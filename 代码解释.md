# 代码解释

这个文件会简单的解释代码。

tiny-LightGbm是cpython混合编程，目前c++提供的接口主要是以下四个：

[1.LGBM_DatasetCreateFromMat](#jupm1)

[2.LGBM_BoosterCreate](#jump2)

[3.LGBM_BoosterUpdateOneIter](#jump3)

[4.LGBM_BoosterPredictForMat](#jump4)



## <span id="jump1">LGBM_DatasetCreateFromMat</span>

![1576224418157](pic\pic1.png)

以上是DataSet类的架构。

dataloader是辅助类，构造dataset。

![1576224644356](pic\pic2.png)

这里并没有直接放入所有数据，0周围的数据并没有放入。

针对每一个feature构造一个BinMapper。每一个BinMapper里维护一个bin_upper_bound_，这里有个default_bin。

![1576224736996](pic\pic3.png)

default_bin是放在value_to_bin（0），即对于不同的feature，其取值不一样。

有了BinMapper之后，构造featuregroup。featuregroup里面维护bin_mappers_，里面有多个bin_mapper，

因为featuregroup里面有多个feature。EFB这里计算feature的范围也不是数值大小，而是bin的数量。

例如feature1：bin（0-10），feature2：bin（10-20）

这里有个细节操作，如果default_bin是在0这个位置，那么要-=1。

![1576224848068](pic\pic4.png)

构造了featuregroup之后，需要投入数据。feature_group的另外一个类，DenseBin。

这个类维护一个data_，data_[i] = j。j代表bin的index。

投入数据的时候，并不是所有的数据都进去，因为默认的bin就是0。

注意的是，bin0是需要特殊处理的，最初所有数据都放在bin0里面，随后将数据分发出去

（这里的data_的bin的个数，是比bin_upper_bound_多一个，bin0）

![1576224912980](pic\pic5.png)

以上的操作，根据我的理解是对于不同的feature可以得到相同的结果。

比如feature1，其值有正有负，那么default_bin就到了非0的位置，添加数据的bin可以为1。（bin=0，bin_offsets = 1，那么bin = 1）

又如feature2，其值只有正，那么default_bin就是0的位置，添加的数据的bin也是1。（bin=1，bin_offsets =1,bin-=1 , 那么bin=1）

保持了一致。



## <span id="jump2">LGBM_BoosterCreate</span>

![1576226480461](pic\pic6.png)

这个函数主要是利用DataSet类做一些训练之前的初始化工作。即初始化Booster函数。

初始化工作如下：

1.DataSet直接传入，const。

2.创建目标函数和评估函数。

3.创建boosting ， 即GBDT（派生类）。

在GBDT的创建中，最繁复的就是TreeLeaner，即SerialTreeLearner（派生类）的创建。

SerialTreeLearner的创建工作：

1.std::vector<SplitInfo> best_split_per_leaf_;

这个参数是为了存储每个叶子的分裂信息。**即lightgbm是leaf-wise的分割策略，但不代表每次都是分裂右子树。而是每一次分裂会在所有的叶子中去比较怎样分裂能最优化。**因此极限情况下，lightgbm完全有可能和xgboost呈现一样的树结构。

2.std::unique_ptr<LeafSplits> smaller_leaf_splits_;

这个参数或者LeafSplits这个类存储了当前叶子上的信息。包括叶子上的数据量，gradients的总量，hessian的总量。

3.std::unique_ptr<DataPartition> data_partition_;

这个参数保留了全局的信息，比如所有叶子的总量，比如叶子上数据的index信息等等。

4.HistogramPool histogram_pool_;

这是最复杂的参数，保存了大量的信息。例如HistogramPool 类中，有：

std::vector < std::unique_ptr<FeatureHistogram[] > >  pool_;

也有HistogramBinEntry* data_;保存每个bin里的一二阶导和数据量；

即pool_保存了所有的叶子，同时每一片叶子保存了所有的feature。

这个参数也会保存feature的一些信息，例如num_bin，或者default_bin，体现在其下的std::vector <FeatureMetainfo> feature_metas_;





## <span id="jump3">LGBM_BoosterUpdateOneIter</span>

train操作会一直从booster传递到GBDT手上。注意到该函数只训练一次。

训练之前会先计算当前的一阶和二阶导数，这里使用L2Regression，二阶导是常数。score初始化为0，当然在当前的任务（回归）下，可以考虑使用平均值去代替0值进行训练，可能会加快训练速度。

![1576229428592](C:\Users\32002\Desktop\tiny_lightgbm\pic\pic7.png)

如下，并不是所有任务都可以使用平均值

![1576229616578](C:\Users\32002\Desktop\tiny_lightgbm\pic\pic8.png)

同时需要注意的是，回归问题或者二分类的问题，每次训练都只需要一棵树。参数是num_tree_per_iteration_；这个参数和num_class是相同的。即如果是多分类问题，在每一轮的训练中，需要训练多棵树。对每个class分开训练。

train函数继续从GBDT手上交到SerialTreeLearner手上。

在开始新的树的训练之前，需要初始化一些参数，例如之前提到的best_split_per_leaf_，smaller_leaf_splits_，data_partition_等等。

每一轮的训练是为了训练一颗新树，这颗树也需要多次训练，有num_leaves来控制。

刚开始时只有根节点，将根节点赋予 FeatureHistogram* smaller_leaf_histogram_array_;

然后构造histogram，即对于这个根节点的所有feature（**注意如果使用了EFB，这里的feature就是合并之后的feature**）构造histogram，需要得到其总的一阶二阶导和数据量。

接下来，就可以针对根节点的所有feature去找到最佳分裂的feature（**这里的feature是原始的feature，没有合并**）。

根节点最开始也是叶子，leaf = 0，分裂之后，左孩子继承0，右孩子得到1（num_leaves - 1）。

根据找到的分裂点，开始分裂树，tree类得到分裂函数，并记录分裂点信息。例如thresh，左右孩子的输出值，父节点的关系等等。

其中需要注意，一个叶子节点的输出并不是叶子节点中所有数据的平均值，而是需要公式计算（陈天奇xgboost讲解）。算方式如下：

![1576230626386](C:\Users\32002\Desktop\tiny_lightgbm\pic\pic9.png)



第二次训练时就有了左右子树，根据左右子树的数据量大小，我们将左树设置为数据量比较多的那一边。

然后为新的左右叶子重新构造histogram（**注意只为smaller这边构造histogram，即只为右边叶子构造，因为可以直接利用父节点的histogram的信息，减去smaller的histogram，得到larger的histogram，减少运算量**），左右叶子都会去寻找最佳分裂点。左右叶子都会计算其分裂之后的熵的增加情况，然后分裂熵增最大的那一边。当然，熵增比较小的这边会保存起来，以后在合适的情况下，可能继续分裂。

之后的训练同理，只不过best_split_per_leaf会越来越大，因为叶子越来越多。最大size即为num_leaves。

最后得到一棵树（num_class = 1）。如果树的有分裂，即叶子大于1，就去更新score_updater。学习率（learning rate）在这里使用。如果有衰减率（shrinkage rate），也是在这里使用。

![1576234534524](C:\Users\32002\Desktop\tiny_lightgbm\pic\pic10.png)

多次重复epoch，得到多个新的树。当然，每棵树开始时计算gradients和hessian会不同，因为boosting方法是建立在“残差”上面，即score - label，而每个新的epoch的score会有所变化。

## <span id="jump4">LGBM_BoosterPredictForMat</span>

